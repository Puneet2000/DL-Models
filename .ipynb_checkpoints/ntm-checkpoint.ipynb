{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "def split_cols(mat, lengths):\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory for NTM's\n",
    "class NTMMemory(nn.Module):\n",
    "    def __init__(self,N,M):\n",
    "        super(NTMMemory,self).__init__()\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.register_buffer('mem_bias', torch.Tensor(N, M))\n",
    "        std = 1 / (np.sqrt(N + M))\n",
    "        nn.init.uniform_(self.mem_bias, -std, std)\n",
    "    \n",
    "    def reset(self,bs):\n",
    "        self.bs = bs\n",
    "        self.memory = self.mem_bias.clone().repeat(bs,1,1)\n",
    "    \n",
    "    def size(self):\n",
    "        return self.N , self.M\n",
    "    \n",
    "    def read(self,weights):\n",
    "        return torch.matmul(weights.unsqueeze(1),self.memory).squeeze(1)\n",
    "    \n",
    "    def write(self,w,e,a):\n",
    "        self.prev_mem = self.memory\n",
    "        self.memory = torch.Tensor(self.bs, self.N, self.M)\n",
    "        erase = torch.matmul(w.unsqueeze(-1),e.unsqueeze(1))\n",
    "        add = torch.matmul(w.unsqueeze(-1),a.unsqueeze(1))\n",
    "        self.memory = self.prev_mem*(1-erase) + add\n",
    "    \n",
    "    def address(self,key_vector,beta,g,shift_weight,gamma,w_prev):\n",
    "        content_w = self.cosine_similarity(key_vector,beta)\n",
    "        \n",
    "        gated_w = self.interpolate(w_prev,content_w,g)\n",
    "        w = self.conv_shift(gated_w,shift_weight)\n",
    "        w = self.sharpen(w,gamma)\n",
    "        return w\n",
    "    \n",
    "    def cosine_similarity(self,key_vector,beta):\n",
    "        key_vector = key_vector.view(self.bs,1,-1)\n",
    "        w = F.softmax(beta*F.cosine_similarity(self.memory + 1e-16, key_vector + 1e-16 , dim=-1),dim=-1)\n",
    "        return w\n",
    "    \n",
    "    def interpolate(self,w_prev,content_w,g):\n",
    "        return g*content_w + (1-g)*w_prev\n",
    "    \n",
    "    def conv_shift(self,gated_w,shift_weight):\n",
    "        res = torch.zeros(gated_w.size())\n",
    "        for b in range(self.bs):\n",
    "            res[b]= convolve(gated_w[b],shift_weight[b])\n",
    "        return res\n",
    "    \n",
    "    def sharpen(self,w,gamma):\n",
    "        w = w**gamma\n",
    "        w = torch.div(w,torch.sum(w,dim=1).view(-1,1)+1e-16)\n",
    "        return w\n",
    "        \n",
    "def convolve(w, s):\n",
    "    assert s.size(0) == 3\n",
    "    t = torch.cat([w[-1:], w, w[:1]])\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM controller using LSTM's\n",
    "class LSTMController(nn.Module):\n",
    "    def __init__(self,n_inputs,n_outputs,n_layers):\n",
    "        super(LSTMController,self).__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_inputs,n_outputs,n_layers)\n",
    "        self.lstm_hidden_bias = Parameter(torch.randn(self.n_layers, 1, self.n_outputs) * 0.05)\n",
    "        self.lstm_cellstate_bias = Parameter(torch.randn(self.n_layers, 1, self.n_outputs) * 0.05)\n",
    "        \n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        hidden_state = self.lstm_hidden_bias.clone().repeat(1,bs,1)\n",
    "        cell_state = self.lstm_cellstate_bias.clone().repeat(1,bs,1)\n",
    "        return hidden_state , cell_state\n",
    "    \n",
    "    def reset_params(self):\n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant_(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.n_inputs +  self.n_outputs))\n",
    "                nn.init.uniform_(p, -stdev, stdev)\n",
    "    \n",
    "    def size(self):\n",
    "        return self.n_inputs, self.n_outputs\n",
    "    \n",
    "    def forward(self,x,prev_state):\n",
    "        x = x.unsqueeze(0)\n",
    "        out , state = self.lstm(x,prev_state)\n",
    "        return out.squeeze(0),state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM read and write heads\n",
    "class NTMReadHead(nn.Module):\n",
    "    def __init__(self,memory,controller_size):\n",
    "        super(NTMReadHead,self).__init__()\n",
    "        self.memory = memory\n",
    "        self.N , self.M = memory.size()\n",
    "        self.controller_size = controller_size\n",
    "        self.read_lengths = [self.M,1,1,3,1]\n",
    "        self.fc_read = nn.Linear(controller_size,sum(self.read_lengths))\n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        return torch.zeros(bs,self.N)\n",
    "    \n",
    "    def reset_params(self):\n",
    "        nn.init.xavier_uniform_(self.fc_read.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_read.bias, std=0.01)\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def address_memory(self, k, β, g, s, γ, w_prev):\n",
    "        k = k.clone()\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "\n",
    "        w = self.memory.address(k, β, g, s, γ, w_prev)\n",
    "        return w\n",
    "    \n",
    "    def forward(self, embeddings, w_prev):\n",
    "        o = self.fc_read(embeddings)\n",
    "        k, β, g, s, γ = split_cols(o, self.read_lengths)\n",
    "        w = self.address_memory(k, β, g, s, γ, w_prev)\n",
    "        r = self.memory.read(w)\n",
    "\n",
    "        return r, w\n",
    "    \n",
    "    \n",
    "class NTMWriteHead(nn.Module):\n",
    "    def __init__(self,memory,controller_size):\n",
    "        super(NTMWriteHead,self).__init__()\n",
    "        self.memory = memory\n",
    "        self.N , self.M = memory.size()\n",
    "        self.controller_size = controller_size\n",
    "        self.write_lengths = [self.M,1,1,3,1,self.M,self.M]\n",
    "        self.fc_write = nn.Linear(controller_size,sum(self.write_lengths))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        return torch.zeros(bs,self.N)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.fc_write.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_write.bias, std=0.01)\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def address_memory(self, k, β, g, s, γ, w_prev):\n",
    "        k = k.clone()\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "\n",
    "        w = self.memory.address(k, β, g, s, γ, w_prev)\n",
    "        return w\n",
    "    \n",
    "    def forward(self, embeddings, w_prev):\n",
    "        o = self.fc_write(embeddings)\n",
    "        k, β, g, s, γ , e,a = split_cols(o, self.write_lengths)\n",
    "        e = F.sigmoid(e)\n",
    "        w = self.address_memory(k, β, g, s, γ, w_prev)\n",
    "        self.memory.write(w, e, a)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM\n",
    "class NTM(nn.Module):\n",
    "    def __init__(self,n_inputs,n_outputs,controller,memory,heads):\n",
    "        super(NTM,self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.controller = controller\n",
    "        self.memory = memory\n",
    "        self.heads = heads\n",
    "        self.N , self.M = memory.size()\n",
    "        _,self.controller_size = controller.size()\n",
    "        \n",
    "        self.n_read_heads =0 \n",
    "        self.init_r = []\n",
    "        for head in heads:\n",
    "            if head.is_read_head():\n",
    "                init_r_bias = torch.randn(1,self.M)*0.01\n",
    "                self.register_buffer(\"read{}_bias\".format(self.n_read_heads), init_r_bias.data)\n",
    "                self.init_r += [init_r_bias]\n",
    "                self.n_read_heads += 1\n",
    "        self.fc = nn.Linear(self.controller_size+self.n_read_heads*self.M,n_outputs)\n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        init_r = [r.clone().repeat(bs,1) for r in  self.init_r]\n",
    "        controller_state = self.controller.create_new_state(bs)\n",
    "        heads_state = [head.create_new_state(bs) for head in self.heads]\n",
    "        \n",
    "        return init_r , controller_state , heads_state\n",
    "    \n",
    "    def reset_params(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "        nn.init.normal_(self.fc.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        prev_reads, prev_controller_state, prev_heads_states = prev_state\n",
    "        inp = torch.cat([x] + prev_reads, dim=1)\n",
    "        controller_outp, controller_state = self.controller(inp, prev_controller_state)\n",
    "        reads = []\n",
    "        heads_states = []\n",
    "        for head, prev_head_state in zip(self.heads, prev_heads_states):\n",
    "            if head.is_read_head():\n",
    "                r, head_state = head(controller_outp, prev_head_state)\n",
    "                reads += [r]\n",
    "            else:\n",
    "                head_state = head(controller_outp, prev_head_state)\n",
    "            heads_states += [head_state]\n",
    "\n",
    "        inp2 = torch.cat([controller_outp] + reads, dim=1)\n",
    "        o = F.sigmoid(self.fc(inp2))\n",
    "\n",
    "        state = (reads, controller_state, heads_states)\n",
    "        return o, state     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedNTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs,\n",
    "                 controller_size, controller_layers, num_heads, N, M):\n",
    "        super(EncapsulatedNTM, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.controller_size = controller_size\n",
    "        self.controller_layers = controller_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "        memory = NTMMemory(N, M)\n",
    "        controller = LSTMController((num_inputs + M*num_heads), controller_size, controller_layers)\n",
    "        heads = nn.ModuleList([])\n",
    "        for i in range(num_heads):\n",
    "            heads += [\n",
    "                NTMReadHead(memory, controller_size),\n",
    "                NTMWriteHead(memory, controller_size)\n",
    "            ]\n",
    "\n",
    "        self.ntm = NTM(num_inputs, num_outputs, controller, memory, heads)\n",
    "        self.memory = memory\n",
    "\n",
    "    def init_sequence(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory.reset(batch_size)\n",
    "        self.previous_state = self.ntm.create_new_state(batch_size)\n",
    "\n",
    "    def forward(self, x=None):\n",
    "        if x is None:\n",
    "            x = torch.zeros(self.batch_size, self.num_inputs)\n",
    "\n",
    "        o, self.previous_state = self.ntm(x, self.previous_state)\n",
    "        return o, self.previous_state\n",
    "\n",
    "    def calculate_num_params(self):\n",
    "        num_params = 0\n",
    "        for p in self.parameters():\n",
    "            num_params += p.data.view(-1).size(0)\n",
    "        return num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(net):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-10, 10)\n",
    "\n",
    "def train_batch(net, criterion, optimizer,X,Y):\n",
    "    optimizer.zero_grad()\n",
    "    input_seq_len = X.size(0)\n",
    "    output_seq_len,bs, _ = Y.size()\n",
    "    \n",
    "    net.init_sequence(bs)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        net(X[i])\n",
    "        \n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(output_seq_len):\n",
    "        y_out[i],_ =  net()\n",
    "    \n",
    "    loss = criterion(y_out,Y)\n",
    "    loss.backward()\n",
    "    clip_grads(net)\n",
    "    optimizer.step()\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "    return loss.item(), cost.item() / bs\n",
    "\n",
    "def evaluate(net, criterion, X, Y):\n",
    "    \"\"\"Evaluate a single batch (without training).\"\"\"\n",
    "    inp_seq_len = X.size(0)\n",
    "    outp_seq_len, batch_size, _ = Y.size()\n",
    "\n",
    "    # New sequence\n",
    "    net.init_sequence(batch_size)\n",
    "\n",
    "    # Feed the sequence + delimiter\n",
    "    states = []\n",
    "    for i in range(inp_seq_len):\n",
    "        o, state = net(X[i])\n",
    "        states += [state]\n",
    "\n",
    "    # Read the output (no input given)\n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(outp_seq_len):\n",
    "        y_out[i], state = net()\n",
    "        states += [state]\n",
    "\n",
    "    loss = criterion(y_out, Y)\n",
    "\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    # The cost is the number of error bits per sequence\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "\n",
    "    result = {\n",
    "        'loss': loss.data[0],\n",
    "        'cost': cost / batch_size,\n",
    "        'y_out': y_out,\n",
    "        'y_out_binarized': y_out_binarized,\n",
    "        'states': states\n",
    "    }\n",
    "    return result\n",
    "    \n",
    "def train_model(num_batches,batch_size,dataloader,net, criterion,optimizer):\n",
    "    print(\"Training model for %d batches (batch_size=%d)...\",num_batches, batch_size)\n",
    "    losses = []\n",
    "    costs = []\n",
    "    seq_lengths = []\n",
    "\n",
    "    for batch_num, x, y in dataloader:\n",
    "        x,y  = x.cuda() , y.cuda()\n",
    "        loss, cost = train_batch(net, criterion, optimizer, x, y)\n",
    "        losses += [loss]\n",
    "        costs += [cost]\n",
    "        seq_lengths += [y.size(0)]\n",
    "        if batch_num % 100 == 0:\n",
    "            print(\"Batch %d Loss: %.6f Cost: %.2f\",batch_num, loss, cost)\n",
    "\n",
    "    print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for %d batches (batch_size=%d)... 50000 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch %d Loss: %.6f Cost: %.2f 100 0.6879629492759705 47.0\n",
      "Batch %d Loss: %.6f Cost: %.2f 200 0.6710794568061829 56.0\n",
      "Batch %d Loss: %.6f Cost: %.2f 300 0.615348756313324 17.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4dc0afa16ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3b935b947932>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(num_batches, batch_size, dataloader, net, criterion, optimizer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3b935b947932>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(net, criterion, optimizer, X, Y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mclip_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def dataloader(num_batches,batch_size,seq_width,min_len,max_len):\n",
    "    for batch_num in range(num_batches):\n",
    "\n",
    "        seq_len = random.randint(min_len, max_len)\n",
    "        seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n",
    "        seq = torch.from_numpy(seq)\n",
    "\n",
    "        inp = torch.zeros(seq_len + 1, batch_size, seq_width + 1)\n",
    "        inp[:seq_len, :, :seq_width] = seq\n",
    "        inp[seq_len, :, seq_width] = 1.0\n",
    "        outp = seq.clone()\n",
    "        yield batch_num+1, inp.float(), outp.float()\n",
    "\n",
    "\n",
    "net = EncapsulatedNTM(9, 8,100, 1,1,128, 20)\n",
    "net = net.cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(net.parameters(),momentum=0.9, alpha=0.95,lr=0.0001)\n",
    "data = dataloader(50000, 1,8,1, 20)\n",
    "train_model(50000,1,data,net,criterion,optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
