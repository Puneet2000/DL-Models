{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063\n"
     ]
    }
   ],
   "source": [
    "def dataset(filename,m_common=3050):\n",
    "    lines = open(filename, encoding='utf-8').read()\n",
    "    n_words = []\n",
    "    sentences = sent_tokenize(lines)\n",
    "    pro_sentences=[]\n",
    "    input_data=[]\n",
    "    output_data=[]\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        n_words+=words\n",
    "        words = ['<START>'] + words + ['<END>']\n",
    "        pro_sentences.append(words) \n",
    "    fdist1 = nltk.FreqDist(n_words)\n",
    "    print(len(fdist1))\n",
    "    n_words = ['<START>']+[word for (word,freq) in fdist1.most_common(m_common)]+['<UNKNOWN>','<END>']\n",
    "    n_tokens = len(n_words)\n",
    "    for sentence in pro_sentences:\n",
    "        length = len(sentence)\n",
    "        indata = torch.zeros(length-1,n_tokens)\n",
    "        outdata = []\n",
    "        for i in range(length-1):\n",
    "            indata[i][n_words.index(sentence[i]) if sentence[i] in n_words else n_words.index('<UNKNOWN>')]=1\n",
    "        for i in range(length-1):\n",
    "            outdata.append([n_words.index(sentence[i+1]) if sentence[i+1] in n_words else n_words.index('<UNKNOWN>')])\n",
    "        input_data.append(indata)\n",
    "        output_data.append(outdata)\n",
    "    return input_data , output_data , n_words  ,n_tokens \n",
    "input_tensors , target_tensors , word_list ,n_tokens = dataset('./shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self,input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 0)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 0)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.softmax(output)\n",
    "        return output.view(1,-1), hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "rnn = RNN(n_tokens,200,n_tokens)\n",
    "rnn = rnn.cuda()\n",
    "optimizer = optim.Adam(rnn.parameters(),lr =0.0001)\n",
    "n_epochs=120\n",
    "n_samples = len(input_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infero/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 is  6.688699245452881\n",
      "Loss at epoch 1 is  6.0889058113098145\n",
      "Loss at epoch 2 is  5.928800582885742\n",
      "Loss at epoch 3 is  5.781315326690674\n",
      "Loss at epoch 4 is  5.635859489440918\n",
      "Loss at epoch 5 is  5.485524654388428\n",
      "Loss at epoch 6 is  5.32078218460083\n",
      "Loss at epoch 7 is  5.146284103393555\n",
      "Loss at epoch 8 is  4.970675468444824\n",
      "Loss at epoch 9 is  4.793716907501221\n",
      "Loss at epoch 10 is  4.612541198730469\n",
      "Loss at epoch 11 is  4.430445671081543\n",
      "Loss at epoch 12 is  4.249372959136963\n",
      "Loss at epoch 13 is  4.0687079429626465\n",
      "Loss at epoch 14 is  3.8952548503875732\n",
      "Loss at epoch 15 is  3.729579210281372\n",
      "Loss at epoch 16 is  3.5515048503875732\n",
      "Loss at epoch 17 is  3.373509645462036\n",
      "Loss at epoch 18 is  3.2023913860321045\n",
      "Loss at epoch 19 is  3.0350754261016846\n",
      "Loss at epoch 20 is  2.8728411197662354\n",
      "Loss at epoch 21 is  2.7147603034973145\n",
      "Loss at epoch 22 is  2.5625600814819336\n",
      "Loss at epoch 23 is  2.4194436073303223\n",
      "Loss at epoch 24 is  2.287484884262085\n",
      "Loss at epoch 25 is  2.1734979152679443\n",
      "Loss at epoch 26 is  2.0547070503234863\n",
      "Loss at epoch 27 is  1.9295709133148193\n",
      "Loss at epoch 28 is  1.8029175996780396\n",
      "Loss at epoch 29 is  1.691253900527954\n",
      "Loss at epoch 30 is  1.5891987085342407\n",
      "Loss at epoch 31 is  1.4920967817306519\n",
      "Loss at epoch 32 is  1.3987910747528076\n",
      "Loss at epoch 33 is  1.312153935432434\n",
      "Loss at epoch 34 is  1.2434353828430176\n",
      "Loss at epoch 35 is  1.1670196056365967\n",
      "Loss at epoch 36 is  1.091440200805664\n",
      "Loss at epoch 37 is  1.0202784538269043\n",
      "Loss at epoch 38 is  0.9536110162734985\n",
      "Loss at epoch 39 is  0.89570152759552\n",
      "Loss at epoch 40 is  0.841498851776123\n",
      "Loss at epoch 41 is  0.7869993448257446\n",
      "Loss at epoch 42 is  0.7371041774749756\n",
      "Loss at epoch 43 is  0.6906722784042358\n",
      "Loss at epoch 44 is  0.6583317518234253\n",
      "Loss at epoch 45 is  0.6193446516990662\n",
      "Loss at epoch 46 is  0.5798695087432861\n",
      "Loss at epoch 47 is  0.5481854677200317\n",
      "Loss at epoch 48 is  0.5126714706420898\n",
      "Loss at epoch 49 is  0.48353657126426697\n",
      "Loss at epoch 50 is  0.4555208086967468\n",
      "Loss at epoch 51 is  0.42842191457748413\n",
      "Loss at epoch 52 is  0.4055631458759308\n",
      "Loss at epoch 53 is  0.38337427377700806\n",
      "Loss at epoch 54 is  0.36534255743026733\n",
      "Loss at epoch 55 is  0.3530521094799042\n",
      "Loss at epoch 56 is  0.3447813391685486\n",
      "Loss at epoch 57 is  0.3333037495613098\n",
      "Loss at epoch 58 is  0.319277286529541\n",
      "Loss at epoch 59 is  0.3056908845901489\n",
      "Loss at epoch 60 is  0.2897418737411499\n",
      "Loss at epoch 61 is  0.27710890769958496\n",
      "Loss at epoch 62 is  0.26648133993148804\n",
      "Loss at epoch 63 is  0.25881099700927734\n",
      "Loss at epoch 64 is  0.25427761673927307\n",
      "Loss at epoch 65 is  0.25132089853286743\n",
      "Loss at epoch 66 is  0.2647320330142975\n",
      "Loss at epoch 67 is  0.28220468759536743\n",
      "Loss at epoch 68 is  0.26259857416152954\n",
      "Loss at epoch 69 is  0.24234716594219208\n",
      "Loss at epoch 70 is  0.2345128357410431\n",
      "Loss at epoch 71 is  0.23100069165229797\n",
      "Loss at epoch 72 is  0.22849571704864502\n",
      "Loss at epoch 73 is  0.2274095118045807\n",
      "Loss at epoch 74 is  0.2260277420282364\n",
      "Loss at epoch 75 is  0.22681862115859985\n",
      "Loss at epoch 76 is  0.2573135495185852\n",
      "Loss at epoch 77 is  0.25036558508872986\n",
      "Loss at epoch 78 is  0.23251205682754517\n",
      "Loss at epoch 79 is  0.22452077269554138\n",
      "Loss at epoch 80 is  0.22163239121437073\n",
      "Loss at epoch 81 is  0.2211989313364029\n",
      "Loss at epoch 82 is  0.22014932334423065\n",
      "Loss at epoch 83 is  0.22029365599155426\n",
      "Loss at epoch 84 is  0.21937738358974457\n",
      "Loss at epoch 85 is  0.2196185141801834\n",
      "Loss at epoch 86 is  0.21876509487628937\n",
      "Loss at epoch 87 is  0.21920214593410492\n",
      "Loss at epoch 88 is  0.22964000701904297\n",
      "Loss at epoch 89 is  0.2598415017127991\n",
      "Loss at epoch 90 is  0.2280721217393875\n",
      "Loss at epoch 91 is  0.21858367323875427\n",
      "Loss at epoch 92 is  0.21648113429546356\n",
      "Loss at epoch 93 is  0.21682095527648926\n",
      "Loss at epoch 94 is  0.21607355773448944\n",
      "Loss at epoch 95 is  0.2167799025774002\n",
      "Loss at epoch 96 is  0.21603228151798248\n",
      "Loss at epoch 97 is  0.21673478186130524\n",
      "Loss at epoch 98 is  0.21592888236045837\n",
      "Loss at epoch 99 is  0.21661007404327393\n",
      "Loss at epoch 100 is  0.2157406210899353\n",
      "Loss at epoch 101 is  0.216414675116539\n",
      "Loss at epoch 102 is  0.2155904918909073\n",
      "Loss at epoch 103 is  0.24249298870563507\n",
      "Loss at epoch 104 is  0.2279074639081955\n",
      "Loss at epoch 105 is  0.2176012396812439\n",
      "Loss at epoch 106 is  0.21422424912452698\n",
      "Loss at epoch 107 is  0.21370649337768555\n",
      "Loss at epoch 108 is  0.21429099142551422\n",
      "Loss at epoch 109 is  0.2138165384531021\n",
      "Loss at epoch 110 is  0.2144831418991089\n",
      "Loss at epoch 111 is  0.21392874419689178\n",
      "Loss at epoch 112 is  0.21457317471504211\n",
      "Loss at epoch 113 is  0.21394915878772736\n",
      "Loss at epoch 114 is  0.214547798037529\n",
      "Loss at epoch 115 is  0.21387135982513428\n",
      "Loss at epoch 116 is  0.2144322544336319\n",
      "Loss at epoch 117 is  0.23611563444137573\n",
      "Loss at epoch 118 is  0.22403490543365479\n",
      "Loss at epoch 119 is  0.21359561383724213\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    epoch_loss =0\n",
    "    for (i,sentence_tensor) in enumerate(input_tensors):\n",
    "        optimizer.zero_grad()\n",
    "        sen_length = len(sentence_tensor)\n",
    "        hidden = rnn.initHidden()\n",
    "        hidden = hidden.cuda()\n",
    "        sen_loss =0\n",
    "        for (j,word_tensor) in enumerate(sentence_tensor):\n",
    "            #print(word_tensor)\n",
    "            output,hidden = rnn(word_tensor.cuda(),hidden)\n",
    "            #print(output)\n",
    "            l=criterion(output,torch.LongTensor(target_tensors[i][j]).cuda())\n",
    "            sen_loss+=l\n",
    "        epoch_loss+= sen_loss/sen_length\n",
    "        sen_loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = epoch_loss/n_samples\n",
    "    print(\"Loss at epoch {0} is \".format(epoch),float(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou art too dear for To possessing , And like enough thou know'st thy estimate , The charter of thy worth gives thee releasing : My bonds in thee are all determinate . <END>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infero/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "start_token , end_token = torch.zeros(n_tokens),torch.zeros(n_tokens)\n",
    "start_token[word_list.index('which')]=1\n",
    "end_token[word_list.index('<END>')]=1\n",
    "#start_token , end_token = start_token.cuda() , end_token.cuda()\n",
    "output_sentence=[]\n",
    "hidden = rnn.initHidden()\n",
    "#hidden = hidden.cuda()\n",
    "rnn = rnn.cpu()\n",
    "while torch.equal(start_token,end_token)==False:\n",
    "    output,hidden = rnn(start_token,hidden)\n",
    "    _,index = torch.max(output[0],0)\n",
    "    output_sentence.append(word_list[index])\n",
    "    start_token = torch.zeros(n_tokens)\n",
    "    start_token[index]=1\n",
    "print(\" \".join(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
