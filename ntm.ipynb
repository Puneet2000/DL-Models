{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def split_cols(mat, lengths):\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory for NTM's\n",
    "class NTMMemory(nn.Module):\n",
    "    def __init__(self,N,M):\n",
    "        super(NTMMemory,self).__init__()\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.register_buffer('mem_bias', torch.Tensor(N, M))\n",
    "        std = 1 / (np.sqrt(N + M))\n",
    "        nn.init.uniform_(self.mem_bias, -std, std)\n",
    "    \n",
    "    def reset(self,bs):\n",
    "        self.bs = bs\n",
    "        self.memory = self.mem_bias.clone().repeat(bs,1,1)\n",
    "    \n",
    "    def size(self):\n",
    "        return self.N , self.M\n",
    "    \n",
    "    def read(self,weights):\n",
    "        return torch.matmul(weights.unsqueeze(1),self.memory).squeeze(1)\n",
    "    \n",
    "    def write(self,w,e,a):\n",
    "        self.prev_mem = self.memory\n",
    "        self.memory = torch.Tensor(self.bs, self.N, self.M)\n",
    "        erase = torch.matmul(w.unsqueeze(-1),e.unsqueeze(1))\n",
    "        add = torch.matmul(w.unsqueeze(-1),a.unsqueeze(1))\n",
    "        self.memory = self.prev_mem*(1-erase) + add\n",
    "    \n",
    "    def address(self,key_vector,beta,g,shift_weight,gamma,w_prev):\n",
    "        content_w = self.cosine_similarity(key_vector,beta)\n",
    "        \n",
    "        gated_w = self.interpolate(w_prev,content_w,g)\n",
    "        w = self.conv_shift(gated_w,shift_weight)\n",
    "        w = self.sharpen(w,gamma)\n",
    "        return w\n",
    "    \n",
    "    def cosine_similarity(self,key_vector,beta):\n",
    "        key_vector = key_vector.view(self.bs,1,-1)\n",
    "        w = F.softmax(beta*F.cosine_similarity(self.memory + 1e-16, key_vector + 1e-16 , dim=-1),dim=-1)\n",
    "        return w\n",
    "    \n",
    "    def interpolate(self,w_prev,content_w,g):\n",
    "        return g*content_w + (1-g)*w_prev\n",
    "    \n",
    "    def conv_shift(self,gated_w,shift_weight):\n",
    "        res = torch.zeros(gated_w.size())\n",
    "        for b in range(self.bs):\n",
    "            res[b]= convolve(gated_w[b],shift_weight[b])\n",
    "        return res\n",
    "    \n",
    "    def sharpen(self,w,gamma):\n",
    "        w = w**gamma\n",
    "        w = torch.div(w,torch.sum(w,dim=1).view(-1,1)+1e-16)\n",
    "        return w\n",
    "        \n",
    "def convolve(w, s):\n",
    "    assert s.size(0) == 3\n",
    "    t = torch.cat([w[-1:], w, w[:1]])\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM controller using LSTM's\n",
    "class LSTMController(nn.Module):\n",
    "    def __init__(self,n_inputs,n_outputs,n_layers):\n",
    "        super(LSTMController,self).__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_inputs,n_outputs,n_layers)\n",
    "        self.lstm_hidden_bias = Parameter(torch.randn(self.n_layers, 1, self.n_outputs) * 0.05)\n",
    "        self.lstm_cellstate_bias = Parameter(torch.randn(self.n_layers, 1, self.n_outputs) * 0.05)\n",
    "        \n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        hidden_state = self.lstm_hidden_bias.clone().repeat(1,bs,1)\n",
    "        cell_state = self.lstm_cellstate_bias.clone().repeat(1,bs,1)\n",
    "        return hidden_state , cell_state\n",
    "    \n",
    "    def reset_params(self):\n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant_(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.n_inputs +  self.n_outputs))\n",
    "                nn.init.uniform_(p, -stdev, stdev)\n",
    "    \n",
    "    def size(self):\n",
    "        return self.n_inputs, self.n_outputs\n",
    "    \n",
    "    def forward(self,x,prev_state):\n",
    "        x = x.unsqueeze(0)\n",
    "        out , state = self.lstm(x,prev_state)\n",
    "        return out.squeeze(0),state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM read and write heads\n",
    "class NTMReadHead(nn.Module):\n",
    "    def __init__(self,memory,controller_size):\n",
    "        super(NTMReadHead,self).__init__()\n",
    "        self.memory = memory\n",
    "        self.N , self.M = memory.size()\n",
    "        self.controller_size = controller_size\n",
    "        self.read_lengths = [self.M,1,1,3,1]\n",
    "        self.fc_read = nn.Linear(controller_size,sum(self.read_lengths))\n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        return torch.zeros(bs,self.N)\n",
    "    \n",
    "    def reset_params(self):\n",
    "        nn.init.xavier_uniform_(self.fc_read.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_read.bias, std=0.01)\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def address_memory(self, k, β, g, s, γ, w_prev):\n",
    "        k = k.clone()\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "\n",
    "        w = self.memory.address(k, β, g, s, γ, w_prev)\n",
    "        return w\n",
    "    \n",
    "    def forward(self, embeddings, w_prev):\n",
    "        o = self.fc_read(embeddings)\n",
    "        k, β, g, s, γ = split_cols(o, self.read_lengths)\n",
    "        w = self.address_memory(k, β, g, s, γ, w_prev)\n",
    "        r = self.memory.read(w)\n",
    "\n",
    "        return r, w\n",
    "    \n",
    "    \n",
    "class NTMWriteHead(nn.Module):\n",
    "    def __init__(self,memory,controller_size):\n",
    "        super(NTMWriteHead,self).__init__()\n",
    "        self.memory = memory\n",
    "        self.N , self.M = memory.size()\n",
    "        self.controller_size = controller_size\n",
    "        self.write_lengths = [self.M,1,1,3,1,self.M,self.M]\n",
    "        self.fc_write = nn.Linear(controller_size,sum(self.write_lengths))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        return torch.zeros(bs,self.N)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.fc_write.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_write.bias, std=0.01)\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def address_memory(self, k, β, g, s, γ, w_prev):\n",
    "        k = k.clone()\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "\n",
    "        w = self.memory.address(k, β, g, s, γ, w_prev)\n",
    "        return w\n",
    "    \n",
    "    def forward(self, embeddings, w_prev):\n",
    "        o = self.fc_write(embeddings)\n",
    "        k, β, g, s, γ , e,a = split_cols(o, self.write_lengths)\n",
    "        e = F.sigmoid(e)\n",
    "        w = self.address_memory(k, β, g, s, γ, w_prev)\n",
    "        self.memory.write(w, e, a)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM\n",
    "class NTM(nn.Module):\n",
    "    def __init__(self,n_inputs,n_outputs,controller,memory,heads):\n",
    "        super(NTM,self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.controller = controller\n",
    "        self.memory = memory\n",
    "        self.heads = heads\n",
    "        self.N , self.M = memory.size()\n",
    "        _,self.controller_size = controller.size()\n",
    "        \n",
    "        self.n_read_heads =0 \n",
    "        self.init_r = []\n",
    "        for head in heads:\n",
    "            if head.is_read_head():\n",
    "                init_r_bias = torch.randn(1,self.M)*0.01\n",
    "                self.register_buffer(\"read{}_bias\".format(self.n_read_heads), init_r_bias.data)\n",
    "                self.init_r += [init_r_bias]\n",
    "                self.n_read_heads += 1\n",
    "        self.fc = nn.Linear(self.controller_size+self.n_read_heads*self.M,n_outputs)\n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        init_r = [r.clone().repeat(bs,1) for r in  self.init_r]\n",
    "        controller_state = self.controller.create_new_state(bs)\n",
    "        heads_state = [head.create_new_state(bs) for head in self.heads]\n",
    "        \n",
    "        return init_r , controller_state , heads_state\n",
    "    \n",
    "    def reset_params(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "        nn.init.normal_(self.fc.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        prev_reads, prev_controller_state, prev_heads_states = prev_state\n",
    "        inp = torch.cat([x] + prev_reads, dim=1)\n",
    "        controller_outp, controller_state = self.controller(inp, prev_controller_state)\n",
    "        reads = []\n",
    "        heads_states = []\n",
    "        for head, prev_head_state in zip(self.heads, prev_heads_states):\n",
    "            if head.is_read_head():\n",
    "                r, head_state = head(controller_outp, prev_head_state)\n",
    "                reads += [r]\n",
    "            else:\n",
    "                head_state = head(controller_outp, prev_head_state)\n",
    "            heads_states += [head_state]\n",
    "\n",
    "        inp2 = torch.cat([controller_outp] + reads, dim=1)\n",
    "        o = F.sigmoid(self.fc(inp2))\n",
    "\n",
    "        state = (reads, controller_state, heads_states)\n",
    "        return o, state     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedNTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs,\n",
    "                 controller_size, controller_layers, num_heads, N, M):\n",
    "        super(EncapsulatedNTM, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.controller_size = controller_size\n",
    "        self.controller_layers = controller_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "        memory = NTMMemory(N, M)\n",
    "        controller = LSTMController((num_inputs + M*num_heads), controller_size, controller_layers)\n",
    "        heads = nn.ModuleList([])\n",
    "        for i in range(num_heads):\n",
    "            heads += [\n",
    "                NTMReadHead(memory, controller_size),\n",
    "                NTMWriteHead(memory, controller_size)\n",
    "            ]\n",
    "\n",
    "        self.ntm = NTM(num_inputs, num_outputs, controller, memory, heads)\n",
    "        self.memory = memory\n",
    "\n",
    "    def init_sequence(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory.reset(batch_size)\n",
    "        self.previous_state = self.ntm.create_new_state(batch_size)\n",
    "\n",
    "    def forward(self, x=None):\n",
    "        if x is None:\n",
    "            x = torch.zeros(self.batch_size, self.num_inputs)\n",
    "\n",
    "        o, self.previous_state = self.ntm(x, self.previous_state)\n",
    "        return o, self.previous_state\n",
    "\n",
    "    def calculate_num_params(self):\n",
    "        num_params = 0\n",
    "        for p in self.parameters():\n",
    "            num_params += p.data.view(-1).size(0)\n",
    "        return num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(net):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-10, 10)\n",
    "\n",
    "def train_batch(net, criterion, optimizer,X,Y):\n",
    "    optimizer.zero_grad()\n",
    "    input_seq_len = X.size(0)\n",
    "    output_seq_len,bs, _ = Y.size()\n",
    "    \n",
    "    net.init_sequence(bs)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        net(X[i])\n",
    "        \n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(output_seq_len):\n",
    "        y_out[i],_ =  net()\n",
    "    \n",
    "    loss = criterion(y_out,Y)\n",
    "    loss.backward()\n",
    "    clip_grads(net)\n",
    "    optimizer.step()\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "    return loss.item(), cost.item() / bs\n",
    "\n",
    "def evaluate(net, criterion, X, Y):\n",
    "    \"\"\"Evaluate a single batch (without training).\"\"\"\n",
    "    inp_seq_len = X.size(0)\n",
    "    outp_seq_len, batch_size, _ = Y.size()\n",
    "\n",
    "    # New sequence\n",
    "    net.init_sequence(batch_size)\n",
    "\n",
    "    # Feed the sequence + delimiter\n",
    "    states = []\n",
    "    for i in range(inp_seq_len):\n",
    "        o, state = net(X[i])\n",
    "        states += [state]\n",
    "\n",
    "    # Read the output (no input given)\n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(outp_seq_len):\n",
    "        y_out[i], state = net()\n",
    "        states += [state]\n",
    "\n",
    "    loss = criterion(y_out, Y)\n",
    "\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    # The cost is the number of error bits per sequence\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "    return y_out_binarized\n",
    "    \n",
    "def train_model(n_epochs,num_batches,batch_size,dataloader,net, criterion,optimizer):\n",
    "    print(\"number of epochs = {2} , Training model for {0} batches (batch_size={1})...\".format(num_batches, batch_size,n_epochs))\n",
    "    losses = []\n",
    "    costs = []\n",
    "    seq_lengths = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_num,x,y in dataloader:\n",
    "            #print(x,x.shape)\n",
    "            loss, cost = train_batch(net, criterion, optimizer, x, y)\n",
    "            losses += [loss]\n",
    "            costs += [cost]\n",
    "            seq_lengths += [y.size(0)]\n",
    "            if batch_num % 100 == 0:\n",
    "                print(\"EPOCH {3} : Batch {0} Loss: {1} Cost: {2}\".format(batch_num, loss, cost,epoch))\n",
    "        #print(dataloader)\n",
    "\n",
    "    print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs = 10 , Training model for 500 batches (batch_size=1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : Batch 100 Loss: 0.6910547614097595 Cost: 52.0\n",
      "EPOCH 0 : Batch 200 Loss: 0.6969127655029297 Cost: 71.0\n",
      "EPOCH 0 : Batch 300 Loss: 0.6202483177185059 Cost: 6.0\n",
      "EPOCH 0 : Batch 400 Loss: 0.7013931274414062 Cost: 62.0\n",
      "EPOCH 0 : Batch 500 Loss: 0.6758865118026733 Cost: 52.0\n",
      "EPOCH 1 : Batch 100 Loss: 0.6942602396011353 Cost: 53.0\n",
      "EPOCH 1 : Batch 200 Loss: 0.6985439658164978 Cost: 82.0\n",
      "EPOCH 1 : Batch 300 Loss: 0.5556929111480713 Cost: 5.0\n",
      "EPOCH 1 : Batch 400 Loss: 0.6902512907981873 Cost: 54.0\n",
      "EPOCH 1 : Batch 500 Loss: 0.6602237820625305 Cost: 53.0\n",
      "EPOCH 2 : Batch 100 Loss: 0.6808363199234009 Cost: 48.0\n",
      "EPOCH 2 : Batch 200 Loss: 0.6829264760017395 Cost: 69.0\n",
      "EPOCH 2 : Batch 300 Loss: 0.49955081939697266 Cost: 6.0\n",
      "EPOCH 2 : Batch 400 Loss: 0.6867748498916626 Cost: 55.0\n",
      "EPOCH 2 : Batch 500 Loss: 0.6559114456176758 Cost: 54.0\n",
      "EPOCH 3 : Batch 100 Loss: 0.6688408255577087 Cost: 45.0\n",
      "EPOCH 3 : Batch 200 Loss: 0.6887193918228149 Cost: 75.0\n",
      "EPOCH 3 : Batch 300 Loss: 0.45360803604125977 Cost: 6.0\n",
      "EPOCH 3 : Batch 400 Loss: 0.6843342781066895 Cost: 57.0\n",
      "EPOCH 3 : Batch 500 Loss: 0.6536780595779419 Cost: 45.0\n",
      "EPOCH 4 : Batch 100 Loss: 0.6626268029212952 Cost: 43.0\n",
      "EPOCH 4 : Batch 200 Loss: 0.6842013001441956 Cost: 76.0\n",
      "EPOCH 4 : Batch 300 Loss: 0.4239291846752167 Cost: 4.0\n",
      "EPOCH 4 : Batch 400 Loss: 0.6733007431030273 Cost: 53.0\n",
      "EPOCH 4 : Batch 500 Loss: 0.6443970799446106 Cost: 45.0\n",
      "EPOCH 5 : Batch 100 Loss: 0.6558336019515991 Cost: 44.0\n",
      "EPOCH 5 : Batch 200 Loss: 0.6817930340766907 Cost: 75.0\n",
      "EPOCH 5 : Batch 300 Loss: 0.4111842215061188 Cost: 5.0\n",
      "EPOCH 5 : Batch 400 Loss: 0.6800273060798645 Cost: 60.0\n",
      "EPOCH 5 : Batch 500 Loss: 0.6497355103492737 Cost: 48.0\n",
      "EPOCH 6 : Batch 100 Loss: 0.6525627970695496 Cost: 42.0\n",
      "EPOCH 6 : Batch 200 Loss: 0.6778814792633057 Cost: 75.0\n",
      "EPOCH 6 : Batch 300 Loss: 0.3895283639431 Cost: 5.0\n",
      "EPOCH 6 : Batch 400 Loss: 0.6763557195663452 Cost: 57.0\n",
      "EPOCH 6 : Batch 500 Loss: 0.6440751552581787 Cost: 44.0\n",
      "EPOCH 7 : Batch 100 Loss: 0.6539764404296875 Cost: 43.0\n",
      "EPOCH 7 : Batch 200 Loss: 0.6737334728240967 Cost: 74.0\n",
      "EPOCH 7 : Batch 300 Loss: 0.3717813491821289 Cost: 4.0\n",
      "EPOCH 7 : Batch 400 Loss: 0.6613398194313049 Cost: 54.0\n",
      "EPOCH 7 : Batch 500 Loss: 0.6371588110923767 Cost: 47.0\n",
      "EPOCH 8 : Batch 100 Loss: 0.6442217230796814 Cost: 41.0\n",
      "EPOCH 8 : Batch 200 Loss: 0.6791743040084839 Cost: 72.0\n",
      "EPOCH 8 : Batch 300 Loss: 0.34138330817222595 Cost: 4.0\n",
      "EPOCH 8 : Batch 400 Loss: 0.6586225032806396 Cost: 52.0\n",
      "EPOCH 8 : Batch 500 Loss: 0.6354004740715027 Cost: 46.0\n",
      "EPOCH 9 : Batch 100 Loss: 0.6498245596885681 Cost: 42.0\n",
      "EPOCH 9 : Batch 200 Loss: 0.6823310852050781 Cost: 74.0\n",
      "EPOCH 9 : Batch 300 Loss: 0.31653866171836853 Cost: 3.0\n",
      "EPOCH 9 : Batch 400 Loss: 0.6555386781692505 Cost: 48.0\n",
      "EPOCH 9 : Batch 500 Loss: 0.6359086632728577 Cost: 47.0\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "def dataloader(num_batches,batch_size,seq_width,min_len,max_len):\n",
    "    dataload =[]\n",
    "    for batch_num in range(num_batches):\n",
    "\n",
    "        seq_len = random.randint(min_len, max_len)\n",
    "        seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n",
    "        seq = torch.from_numpy(seq)\n",
    "\n",
    "        inp = torch.zeros(seq_len + 1, batch_size, seq_width + 1)\n",
    "        inp[:seq_len, :, :seq_width] = seq\n",
    "        inp[seq_len, :, seq_width] = 1.0\n",
    "        outp = seq.clone()\n",
    "        dataload.append((batch_num+1, inp.float(), outp.float()))\n",
    "    return dataload\n",
    "\n",
    "\n",
    "net = EncapsulatedNTM(9, 8,100, 1,1,128, 20)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(net.parameters(),momentum=0.9, alpha=0.95,lr=0.0001)\n",
    "data = dataloader(500, 1,8,1, 20)\n",
    "train_model(10,500,1,data,net,criterion,optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "seq_len=7 \n",
    "batch_size=1\n",
    "seq_width = 20\n",
    "seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n",
    "seq = torch.from_numpy(seq)\n",
    "inp = torch.zeros(seq_len + 1, batch_size, seq_width + 1)\n",
    "inp[:seq_len, :, :seq_width] = seq\n",
    "inp[seq_len, :, seq_width] = 1.0\n",
    "outp = seq.clone()\n",
    "result = evaluate(net,criterion,inp.float(),outp.float())\n",
    "outp = outp.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result= result.squeeze(1)\n",
    "outp = outp.squeeze(1)\n",
    "G = np.zeros((seq_len,seq_width,3))\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_width):\n",
    "        if result[i][j]==1:\n",
    "            G[i][j] = [1,1,1]\n",
    "        else:\n",
    "            G[i][j] = [0,0,0]\n",
    "plt.imshow(G,interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
