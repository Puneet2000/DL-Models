{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "def split_cols(mat, lengths):\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory for NTM's\n",
    "class NTMMemory(nn.Module):\n",
    "    def __init__(self,N,M):\n",
    "        super(NTMMemory,self).__init__()\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.register_buffer('mem_bias', torch.Tensor(N, M))\n",
    "        std = 1 / (np.sqrt(N + M))\n",
    "        nn.init.uniform_(self.mem_bias, -std, std)\n",
    "    \n",
    "    def reset(self,bs):\n",
    "        self.bs = bs\n",
    "        self.memory = self.mem_bias.clone().repeat(bs,1,1)\n",
    "    \n",
    "    def size(self):\n",
    "        return self.N , self.M\n",
    "    \n",
    "    def read(self,weights):\n",
    "        return torch.matmul(weights.unsqueeze(1),self.memory).squeeze(1)\n",
    "    \n",
    "    def write(self,w,e,a):\n",
    "        self.prev_mem = self.memory\n",
    "        self.memory = torch.Tensor(self.bs, self.N, self.M)\n",
    "        erase = torch.matmul(w.unsqueeze(-1),e.unsqueeze(1))\n",
    "        add = torch.matmul(w.unsqueeze(-1),a.unsqueeze(1))\n",
    "        self.memory = self.prev_mem*(1-erase) + add\n",
    "    \n",
    "    def address(self,key_vector,beta,g,shift_weight,gamma,w_prev):\n",
    "        content_w = self.cosine_similarity(key_vector,beta)\n",
    "        \n",
    "        gated_w = self.interpolate(w_prev,content_w,g)\n",
    "        w = self.conv_shift(gated_w,shift_weight)\n",
    "        w = self.sharpen(w,gamma)\n",
    "        return w\n",
    "    \n",
    "    def cosine_similarity(self,key_vector,beta):\n",
    "        key_vector = key_vector.view(self.bs,1,-1)\n",
    "        w = F.softmax(beta*F.cosine_similarity(self.memory + 1e-16, key_vector + 1e-16 , dim=-1),dim=-1)\n",
    "        return w\n",
    "    \n",
    "    def interpolate(self,w_prev,content_w,g):\n",
    "        return g*content_w + (1-g)*w_prev\n",
    "    \n",
    "    def conv_shift(self,gated_w,shift_weight):\n",
    "        res = torch.zeros(gated_w.size())\n",
    "        for b in range(self.bs):\n",
    "            res[b]= convolve(gated_w[b],shift_weight[b])\n",
    "        return res\n",
    "    \n",
    "    def sharpen(self,w,gamma):\n",
    "        w = w**gamma\n",
    "        w = torch.div(w,torch.sum(w,dim=1).view(-1,1)+1e-16)\n",
    "        return w\n",
    "        \n",
    "def convolve(w, s):\n",
    "    assert s.size(0) == 3\n",
    "    t = torch.cat([w[-1:], w, w[:1]])\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM controller using LSTM's\n",
    "class LSTMController(nn.Module):\n",
    "    def __init__(self,n_inputs,n_outputs,n_layers):\n",
    "        super(LSTMController,self).__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_inputs,n_outputs,n_layers)\n",
    "        self.lstm_hidden_bias = Parameter(torch.randn(self.n_layers, 1, self.n_outputs) * 0.05)\n",
    "        self.lstm_cellstate_bias = Parameter(torch.randn(self.n_layers, 1, self.n_outputs) * 0.05)\n",
    "        \n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        hidden_state = self.lstm_hidden_bias.clone().repeat(1,bs,1)\n",
    "        cell_state = self.lstm_cellstate_bias.clone().repeat(1,bs,1)\n",
    "        return hidden_state , cell_state\n",
    "    \n",
    "    def reset_params(self):\n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant_(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.n_inputs +  self.n_outputs))\n",
    "                nn.init.uniform_(p, -stdev, stdev)\n",
    "    \n",
    "    def size(self):\n",
    "        return self.n_inputs, self.n_outputs\n",
    "    \n",
    "    def forward(self,x,prev_state):\n",
    "        x = x.unsqueeze(0)\n",
    "        out , state = self.lstm(x,prev_state)\n",
    "        return out.squeeze(0),state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM read and write heads\n",
    "class NTMReadHead(nn.Module):\n",
    "    def __init__(self,memory,controller_size):\n",
    "        super(NTMReadHead,self).__init__()\n",
    "        self.memory = memory\n",
    "        self.N , self.M = memory.size()\n",
    "        self.controller_size = controller_size\n",
    "        self.read_lengths = [self.M,1,1,3,1]\n",
    "        self.fc_read = nn.Linear(controller_size,sum(self.read_lengths))\n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        return torch.zeros(bs,self.N)\n",
    "    \n",
    "    def reset_params(self):\n",
    "        nn.init.xavier_uniform_(self.fc_read.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_read.bias, std=0.01)\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def address_memory(self, k, β, g, s, γ, w_prev):\n",
    "        k = k.clone()\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "\n",
    "        w = self.memory.address(k, β, g, s, γ, w_prev)\n",
    "        return w\n",
    "    \n",
    "    def forward(self, embeddings, w_prev):\n",
    "        o = self.fc_read(embeddings)\n",
    "        k, β, g, s, γ = split_cols(o, self.read_lengths)\n",
    "        w = self.address_memory(k, β, g, s, γ, w_prev)\n",
    "        r = self.memory.read(w)\n",
    "\n",
    "        return r, w\n",
    "    \n",
    "    \n",
    "class NTMWriteHead(nn.Module):\n",
    "    def __init__(self,memory,controller_size):\n",
    "        super(NTMWriteHead,self).__init__()\n",
    "        self.memory = memory\n",
    "        self.N , self.M = memory.size()\n",
    "        self.controller_size = controller_size\n",
    "        self.write_lengths = [self.M,1,1,3,1,self.M,self.M]\n",
    "        self.fc_write = nn.Linear(controller_size,sum(self.write_lengths))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        return torch.zeros(bs,self.N)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.fc_write.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_write.bias, std=0.01)\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def address_memory(self, k, β, g, s, γ, w_prev):\n",
    "        k = k.clone()\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "\n",
    "        w = self.memory.address(k, β, g, s, γ, w_prev)\n",
    "        return w\n",
    "    \n",
    "    def forward(self, embeddings, w_prev):\n",
    "        o = self.fc_write(embeddings)\n",
    "        k, β, g, s, γ , e,a = split_cols(o, self.write_lengths)\n",
    "        e = F.sigmoid(e)\n",
    "        w = self.address_memory(k, β, g, s, γ, w_prev)\n",
    "        self.memory.write(w, e, a)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM\n",
    "class NTM(nn.Module):\n",
    "    def __init__(self,n_inputs,n_outputs,controller,memory,heads):\n",
    "        super(NTM,self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.controller = controller\n",
    "        self.memory = memory\n",
    "        self.heads = heads\n",
    "        self.N , self.M = memory.size()\n",
    "        _,self.controller_size = controller.size()\n",
    "        \n",
    "        self.n_read_heads =0 \n",
    "        self.init_r = []\n",
    "        for head in heads:\n",
    "            if head.is_read_head():\n",
    "                init_r_bias = torch.randn(1,self.M)*0.01\n",
    "                self.register_buffer(\"read{}_bias\".format(self.n_read_heads), init_r_bias.data)\n",
    "                self.init_r += [init_r_bias]\n",
    "                self.n_read_heads += 1\n",
    "        self.fc = nn.Linear(self.controller_size+self.n_read_heads*self.M,n_outputs)\n",
    "        self.reset_params()\n",
    "    \n",
    "    def create_new_state(self,bs):\n",
    "        init_r = [r.clone().repeat(bs,1) for r in  self.init_r]\n",
    "        controller_state = self.controller.create_new_state(bs)\n",
    "        heads_state = [head.create_new_state(bs) for head in self.heads]\n",
    "        \n",
    "        return init_r , controller_state , heads_state\n",
    "    \n",
    "    def reset_params(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "        nn.init.normal_(self.fc.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        prev_reads, prev_controller_state, prev_heads_states = prev_state\n",
    "        inp = torch.cat([x] + prev_reads, dim=1)\n",
    "        controller_outp, controller_state = self.controller(inp, prev_controller_state)\n",
    "        reads = []\n",
    "        heads_states = []\n",
    "        for head, prev_head_state in zip(self.heads, prev_heads_states):\n",
    "            if head.is_read_head():\n",
    "                r, head_state = head(controller_outp, prev_head_state)\n",
    "                reads += [r]\n",
    "            else:\n",
    "                head_state = head(controller_outp, prev_head_state)\n",
    "            heads_states += [head_state]\n",
    "\n",
    "        inp2 = torch.cat([controller_outp] + reads, dim=1)\n",
    "        o = F.sigmoid(self.fc(inp2))\n",
    "\n",
    "        state = (reads, controller_state, heads_states)\n",
    "        return o, state     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedNTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs,\n",
    "                 controller_size, controller_layers, num_heads, N, M):\n",
    "        super(EncapsulatedNTM, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.controller_size = controller_size\n",
    "        self.controller_layers = controller_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "        memory = NTMMemory(N, M)\n",
    "        controller = LSTMController((num_inputs + M*num_heads), controller_size, controller_layers)\n",
    "        heads = nn.ModuleList([])\n",
    "        for i in range(num_heads):\n",
    "            heads += [\n",
    "                NTMReadHead(memory, controller_size),\n",
    "                NTMWriteHead(memory, controller_size)\n",
    "            ]\n",
    "\n",
    "        self.ntm = NTM(num_inputs, num_outputs, controller, memory, heads)\n",
    "        self.memory = memory\n",
    "\n",
    "    def init_sequence(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory.reset(batch_size)\n",
    "        self.previous_state = self.ntm.create_new_state(batch_size)\n",
    "\n",
    "    def forward(self, x=None):\n",
    "        if x is None:\n",
    "            x = torch.zeros(self.batch_size, self.num_inputs)\n",
    "\n",
    "        o, self.previous_state = self.ntm(x, self.previous_state)\n",
    "        return o, self.previous_state\n",
    "\n",
    "    def calculate_num_params(self):\n",
    "        num_params = 0\n",
    "        for p in self.parameters():\n",
    "            num_params += p.data.view(-1).size(0)\n",
    "        return num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(net):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-10, 10)\n",
    "\n",
    "def train_batch(net, criterion, optimizer,X,Y):\n",
    "    optimizer.zero_grad()\n",
    "    input_seq_len = X.size(0)\n",
    "    output_seq_len,bs, _ = Y.size()\n",
    "    \n",
    "    net.init_sequence(bs)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        net(X[i])\n",
    "        \n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(output_seq_len):\n",
    "        y_out[i],_ =  net()\n",
    "    \n",
    "    loss = criterion(y_out,Y)\n",
    "    loss.backward()\n",
    "    clip_grads(net)\n",
    "    optimizer.step()\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "    return loss.item(), cost.item() / bs\n",
    "\n",
    "def evaluate(net, criterion, X, Y):\n",
    "    \"\"\"Evaluate a single batch (without training).\"\"\"\n",
    "    inp_seq_len = X.size(0)\n",
    "    outp_seq_len, batch_size, _ = Y.size()\n",
    "\n",
    "    # New sequence\n",
    "    net.init_sequence(batch_size)\n",
    "\n",
    "    # Feed the sequence + delimiter\n",
    "    states = []\n",
    "    for i in range(inp_seq_len):\n",
    "        o, state = net(X[i])\n",
    "        states += [state]\n",
    "\n",
    "    # Read the output (no input given)\n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(outp_seq_len):\n",
    "        y_out[i], state = net()\n",
    "        states += [state]\n",
    "\n",
    "    loss = criterion(y_out, Y)\n",
    "\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    # The cost is the number of error bits per sequence\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "    return y_out_binarized\n",
    "    \n",
    "def train_model(n_epochs,num_batches,batch_size,dataloader,net, criterion,optimizer):\n",
    "    print(\"number of epochs = {2} , Training model for {0} batches (batch_size={1})...\".format(num_batches, batch_size,n_epochs))\n",
    "    losses = []\n",
    "    costs = []\n",
    "    seq_lengths = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_num,x,y in dataloader:\n",
    "            #print(x,x.shape)\n",
    "            loss, cost = train_batch(net, criterion, optimizer, x, y)\n",
    "            losses += [loss]\n",
    "            costs += [cost]\n",
    "            seq_lengths += [y.size(0)]\n",
    "            if batch_num % 100 == 0:\n",
    "                print(\"EPOCH {3} : Batch {0} Loss: {1} Cost: {2}\".format(batch_num, loss, cost,epoch))\n",
    "        #print(dataloader)\n",
    "\n",
    "    print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs = 10 , Training model for 500 batches (batch_size=1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : Batch 100 Loss: 0.6910547614097595 Cost: 52.0\n",
      "EPOCH 0 : Batch 200 Loss: 0.6969127655029297 Cost: 71.0\n",
      "EPOCH 0 : Batch 300 Loss: 0.6202483177185059 Cost: 6.0\n",
      "EPOCH 0 : Batch 400 Loss: 0.7013931274414062 Cost: 62.0\n",
      "EPOCH 0 : Batch 500 Loss: 0.6758865118026733 Cost: 52.0\n",
      "EPOCH 1 : Batch 100 Loss: 0.6942602396011353 Cost: 53.0\n",
      "EPOCH 1 : Batch 200 Loss: 0.6985439658164978 Cost: 82.0\n",
      "EPOCH 1 : Batch 300 Loss: 0.5556929111480713 Cost: 5.0\n",
      "EPOCH 1 : Batch 400 Loss: 0.6902512907981873 Cost: 54.0\n",
      "EPOCH 1 : Batch 500 Loss: 0.6602237820625305 Cost: 53.0\n",
      "EPOCH 2 : Batch 100 Loss: 0.6808363199234009 Cost: 48.0\n",
      "EPOCH 2 : Batch 200 Loss: 0.6829264760017395 Cost: 69.0\n",
      "EPOCH 2 : Batch 300 Loss: 0.49955081939697266 Cost: 6.0\n",
      "EPOCH 2 : Batch 400 Loss: 0.6867748498916626 Cost: 55.0\n",
      "EPOCH 2 : Batch 500 Loss: 0.6559114456176758 Cost: 54.0\n",
      "EPOCH 3 : Batch 100 Loss: 0.6688408255577087 Cost: 45.0\n",
      "EPOCH 3 : Batch 200 Loss: 0.6887193918228149 Cost: 75.0\n",
      "EPOCH 3 : Batch 300 Loss: 0.45360803604125977 Cost: 6.0\n",
      "EPOCH 3 : Batch 400 Loss: 0.6843342781066895 Cost: 57.0\n",
      "EPOCH 3 : Batch 500 Loss: 0.6536780595779419 Cost: 45.0\n",
      "EPOCH 4 : Batch 100 Loss: 0.6626268029212952 Cost: 43.0\n",
      "EPOCH 4 : Batch 200 Loss: 0.6842013001441956 Cost: 76.0\n",
      "EPOCH 4 : Batch 300 Loss: 0.4239291846752167 Cost: 4.0\n",
      "EPOCH 4 : Batch 400 Loss: 0.6733007431030273 Cost: 53.0\n",
      "EPOCH 4 : Batch 500 Loss: 0.6443970799446106 Cost: 45.0\n",
      "EPOCH 5 : Batch 100 Loss: 0.6558336019515991 Cost: 44.0\n",
      "EPOCH 5 : Batch 200 Loss: 0.6817930340766907 Cost: 75.0\n",
      "EPOCH 5 : Batch 300 Loss: 0.4111842215061188 Cost: 5.0\n",
      "EPOCH 5 : Batch 400 Loss: 0.6800273060798645 Cost: 60.0\n",
      "EPOCH 5 : Batch 500 Loss: 0.6497355103492737 Cost: 48.0\n",
      "EPOCH 6 : Batch 100 Loss: 0.6525627970695496 Cost: 42.0\n",
      "EPOCH 6 : Batch 200 Loss: 0.6778814792633057 Cost: 75.0\n",
      "EPOCH 6 : Batch 300 Loss: 0.3895283639431 Cost: 5.0\n",
      "EPOCH 6 : Batch 400 Loss: 0.6763557195663452 Cost: 57.0\n",
      "EPOCH 6 : Batch 500 Loss: 0.6440751552581787 Cost: 44.0\n",
      "EPOCH 7 : Batch 100 Loss: 0.6539764404296875 Cost: 43.0\n",
      "EPOCH 7 : Batch 200 Loss: 0.6737334728240967 Cost: 74.0\n",
      "EPOCH 7 : Batch 300 Loss: 0.3717813491821289 Cost: 4.0\n",
      "EPOCH 7 : Batch 400 Loss: 0.6613398194313049 Cost: 54.0\n",
      "EPOCH 7 : Batch 500 Loss: 0.6371588110923767 Cost: 47.0\n",
      "EPOCH 8 : Batch 100 Loss: 0.6442217230796814 Cost: 41.0\n",
      "EPOCH 8 : Batch 200 Loss: 0.6791743040084839 Cost: 72.0\n",
      "EPOCH 8 : Batch 300 Loss: 0.34138330817222595 Cost: 4.0\n",
      "EPOCH 8 : Batch 400 Loss: 0.6586225032806396 Cost: 52.0\n",
      "EPOCH 8 : Batch 500 Loss: 0.6354004740715027 Cost: 46.0\n",
      "EPOCH 9 : Batch 100 Loss: 0.6498245596885681 Cost: 42.0\n",
      "EPOCH 9 : Batch 200 Loss: 0.6823310852050781 Cost: 74.0\n",
      "EPOCH 9 : Batch 300 Loss: 0.31653866171836853 Cost: 3.0\n",
      "EPOCH 9 : Batch 400 Loss: 0.6555386781692505 Cost: 48.0\n",
      "EPOCH 9 : Batch 500 Loss: 0.6359086632728577 Cost: 47.0\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "def dataloader(num_batches,batch_size,seq_width,min_len,max_len):\n",
    "    dataload =[]\n",
    "    for batch_num in range(num_batches):\n",
    "\n",
    "        seq_len = random.randint(min_len, max_len)\n",
    "        seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n",
    "        seq = torch.from_numpy(seq)\n",
    "\n",
    "        inp = torch.zeros(seq_len + 1, batch_size, seq_width + 1)\n",
    "        inp[:seq_len, :, :seq_width] = seq\n",
    "        inp[seq_len, :, seq_width] = 1.0\n",
    "        outp = seq.clone()\n",
    "        dataload.append((batch_num+1, inp.float(), outp.float()))\n",
    "    return dataload\n",
    "\n",
    "\n",
    "net = EncapsulatedNTM(9, 8,100, 1,1,128, 20)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(net.parameters(),momentum=0.9, alpha=0.95,lr=0.0001)\n",
    "data = dataloader(500, 1,8,1, 20)\n",
    "train_model(10,500,1,data,net,criterion,optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 1., 0., 0., 1.]]])\n",
      "tensor([[[1., 0., 0., 0., 1., 0., 0., 1.]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puneet/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "seq_len=1 \n",
    "batch_size=1\n",
    "seq_width = 8\n",
    "seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n",
    "seq = torch.from_numpy(seq)\n",
    "inp = torch.zeros(seq_len + 1, batch_size, seq_width + 1)\n",
    "inp[:seq_len, :, :seq_width] = seq\n",
    "inp[seq_len, :, seq_width] = 1.0\n",
    "outp = seq.clone()\n",
    "result = evaluate(net,criterion,inp.float(),outp.float())\n",
    "print(outp.float())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
